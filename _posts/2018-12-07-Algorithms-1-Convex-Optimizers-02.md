---
title: "最优化方法整理（二）—— Online场景下LR最优化算法"
description: "整理一下Batch/Online场景下，各种常用优化算法知识点"
comments: true
mathjax: true
excerpt_separator: <!--more-->
category: [ML]
tags: [Statistical Learning,Theories,MachineLearning,Convex,Optimizer]
key: ALGO_01_ConvOpt_02
sidebar:
  nav: ml
---

本文接上文（[最优化方法整理（一）——Batch场景下SGD类优化方法](http://libingchen.me/ml/2018/12/03/Algorithms-1-Convex-Optimizers-01.html)），继续整理下在Online场景下，LR算法优化方法，为了获得更大的模型（参数）稀疏性，各个算法所做的一些努力。

主要参考文档是冯杨的![在线最优化求解](http://dongzebo.com/2019/07/19/%E5%9C%A8%E7%BA%BF%E6%9C%80%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3-Online-Optimization-%E5%86%AF%E6%89%AC/) 【没找到最原始文档链接】

<!--more-->

基于上文所提到的那些优化算法，我们可以按需定义合理的目标函数(如LR里面的LogLoss+L1Norm函数)，并得到最优结果。在某些情况下，我们需要模型（参数）具有比较高的稀疏性，在Batch场景下，我们一般通过增加正则化Penalty的方法达到目的。 

但上述方法要求我们每次训练都需要全量的训练数据，当已有累积数据极大时，每次重新训练的成本就随之骤增。同时，在某些大规模场景中，实时数据规模相当大，对模型的实时更新也有了一定要求，如果无法及时地将新的行为、特征加入模型，那么将损失相当一部分效果。

基于对模型`实时性`和`训练成本`两方面的考虑，`流式训练(每次基于已有模型，训练部分最新数据)`便成为一个可选项。但相比于使用全量数据，当模型有比较高的稀疏性要求时，流式训练更新权重所用到的梯度，是基于部分新数据得来的，并未考虑整体训练数据，所以可能无法得到预期的稀疏解（比如L1正则化下的logloss优化场景，在一般情况下我们预期参数有比较高的稀疏性）。

既然`在损失函数中添加正则化惩罚项`的方法，已经不足以让`流式训练`场景下的参数，产生足够的稀疏性，那我们便需要在`优化过程`本身寻找切入点。这`可能`就是一系列在线优化算法产生的初衷吧。

## 0. SGD

随机梯度下降（SGD），只是将一般的梯度下降算法，直接应用到了Online场景，基于部分最新数据得到的梯度，直接被更新到参数之中。具体来说，相对于GD基于全量样本计算梯度，SGD每遇到一个样本，都会计算其梯度，并进行参数更新，以此适用Online增量更新场景。

但直接在Online场景下应用SGD，受到数据被「观测」顺序以及其分布的差异，优化算法在参数空间中的搜索路径无法保证如同GD在全量样本上迭代一样，朝着全局最优前进。如此，在对「模型稀疏性」有要求的场景中，即使加入L1正则惩罚项，也不能保证一定能够得到足够稀疏的解。

## 1. 简单截断法

针对SGD的问题，简单截断法从优化算法本身（而非以往从优化目标函数）出发，按照既定标准，直接将下降梯度截断。其主要出发点是，如果某参数在当前时刻，更新梯度后绝对值过小，那么便认为其对模型贡献度不高，可直接截断，置为0，以此实现增加最终优化结果的稀疏性。具体如下方所示。

在迭代过程中，每$k$次进行一次截断判定操作，其他时刻保持正常的梯度下降更新策略：

$$
W_{t+1} = T(W_{t} - \eta_{t} * G(t), \theta)
$$

$$
T(v, \theta) = 
\begin{cases} 
0, &\text{if} \ v \ \le \theta  \ \text{and} \  \\
v, &\text{otherwise}
\end{cases}
$$

## 2. TG

> 每$k$次做一次截断，但截断的时候，增加一个软化参数$\alpha$，只有当权重小于$\alpha$时截断，而处于$\alpha$与$\theta$之间的权重，仅做降权（scale down）操作


$$
W_{t} = T(W_{t} - \eta_{t} * G(t), \theta, \alpha)
$$

$$
T(v,\theta,\alpha) = 
\begin{cases}
0, &\ \text{if} \ \|v\| < \alpha \\
v - sign(v)*\alpha &\ \text{if} \ \alpha \le \|v\| \le \theta \\
v &\ \text{otherwise}
\end{cases}
$$

## 3. FOBOS

> 先做一步正常的梯度下降操作，在此基础上，找到使「MSE+Norm」损失最小的权重作为下一个时刻的权重。这个操作，相当于先向前做一步迭代，然后再往后考虑下是否有更优的结果

## 4. RDA

> 以上所有算法，只是根据当前时刻的梯度来进行各种操作的判定，没有考虑历史累积梯度均值，对于一些展现频率低、更新幅度较小的参数并不友好，RDA算法将历史累积梯度均值引入，综合考虑「MSE+Norm+累计梯度均值」，找到最优的迭代方向

## 5. FTRL

> FTRL算法结合RDA与FOBOS两种范式，采众之所长，同时考虑历史累积梯度均值以及迭代后向择优，在保持精度的情况下，得到相当不错的稀疏性
